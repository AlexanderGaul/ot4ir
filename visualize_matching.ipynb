{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import pdb\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import sklearn.preprocessing as skprep\n",
    "\n",
    "import ot\n",
    "import sklearn.metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "import torch\n",
    "from torch.utils.model_zoo import load_url\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cirtorch.networks.imageretrievalnet import init_network, extract_interest_points_mac\n",
    "from cirtorch.datasets.datahelpers import cid2filename\n",
    "from cirtorch.datasets.testdataset import configdataset\n",
    "from cirtorch.utils.download import download_train, download_test\n",
    "from cirtorch.utils.whiten import whitenlearn, whitenapply\n",
    "from cirtorch.utils.evaluate import compute_map_and_print\n",
    "from cirtorch.utils.general import get_data_root, htime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Dataset oxford5k directory does not exist. Creating: /home/alexander/Documents/studies/20_ss/guided_research/code/ot4ir/data/test/oxford5k/jpg\n",
      ">> Downloading dataset oxford5k archive oxbuild_images.tgz...\n",
      ">> Extracting dataset oxford5k archive oxbuild_images.tgz...\n",
      ">> Extracted, deleting dataset oxford5k archive oxbuild_images.tgz...\n",
      ">> Downloading dataset oxford5k ground truth file...\n",
      ">> Dataset paris6k directory does not exist. Creating: /home/alexander/Documents/studies/20_ss/guided_research/code/ot4ir/data/test/paris6k/jpg\n",
      ">> Downloading dataset paris6k archive paris_1.tgz...\n",
      ">> Extracting dataset paris6k archive paris_1.tgz...\n",
      ">> Extracted, deleting dataset paris6k archive paris_1.tgz...\n",
      ">> Downloading dataset paris6k archive paris_2.tgz...\n",
      ">> Extracting dataset paris6k archive paris_2.tgz...\n",
      ">> Extracted, deleting dataset paris6k archive paris_2.tgz...\n",
      ">> Downloading dataset paris6k ground truth file...\n",
      ">> Dataset roxford5k directory does not exist. Creating: /home/alexander/Documents/studies/20_ss/guided_research/code/ot4ir/data/test/roxford5k/jpg\n",
      ">> Created symbolic link from oxford5k jpg to roxford5k jpg\n",
      ">> Downloading dataset roxford5k ground truth file...\n",
      ">> Dataset rparis6k directory does not exist. Creating: /home/alexander/Documents/studies/20_ss/guided_research/code/ot4ir/data/test/rparis6k/jpg\n",
      ">> Created symbolic link from paris6k jpg to rparis6k jpg\n",
      ">> Downloading dataset rparis6k ground truth file...\n"
     ]
    }
   ],
   "source": [
    "download_test(get_data_root())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'oxford5k'\n",
    "network_path = 'http://cmp.felk.cvut.cz/cnnimageretrieval/data/networks/retrieval-SfM-120k/rSfM120k-tl-resnet50-gem-w-97bf910.pth'\n",
    "image_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = load_url(network_path, model_dir=os.path.join(get_data_root(), 'networks'))\n",
    "net_params = {}\n",
    "net_params['architecture'] = state['meta']['architecture']\n",
    "net_params['pooling'] = state['meta']['pooling']\n",
    "net_params['local_whitening'] = state['meta'].get('local_whitening', False)\n",
    "net_params['regional'] = state['meta'].get('regional', False)\n",
    "net_params['whitening'] = state['meta'].get('whitening', False)\n",
    "net_params['mean'] = state['meta']['mean']\n",
    "net_params['std'] = state['meta']['std']\n",
    "net_params['pretrained'] = False\n",
    "net = init_network(net_params)\n",
    "net.load_state_dict(state['state_dict'])\n",
    "net.eval()\n",
    "normalize = transforms.Normalize(\n",
    "    mean=net.meta['mean'],\n",
    "    std=net.meta['std']\n",
    ")\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "cfg = configdataset(dataset, os.path.join(get_data_root(), 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matches(ax, image1, image2, keypoints1, keypoints2, matches,\n",
    "                 keypoints_color='k', matches_color=None, only_matches=False,\n",
    "                 alignment='horizontal'):\n",
    "\n",
    "    new_shape1 = list(image1.shape)\n",
    "    new_shape2 = list(image2.shape)\n",
    "\n",
    "    if image1.shape[0] < image2.shape[0]:\n",
    "        new_shape1[0] = image2.shape[0]\n",
    "    elif image1.shape[0] > image2.shape[0]:\n",
    "        new_shape2[0] = image1.shape[0]\n",
    "\n",
    "    if image1.shape[1] < image2.shape[1]:\n",
    "        new_shape1[1] = image2.shape[1]\n",
    "    elif image1.shape[1] > image2.shape[1]:\n",
    "        new_shape2[1] = image1.shape[1]\n",
    "\n",
    "    if new_shape1 != image1.shape:\n",
    "        new_image1 = np.zeros(new_shape1, dtype=image1.dtype)\n",
    "        new_image1[:image1.shape[0], :image1.shape[1]] = image1\n",
    "        image1 = new_image1\n",
    "\n",
    "    if new_shape2 != image2.shape:\n",
    "        new_image2 = np.zeros(new_shape2, dtype=image2.dtype)\n",
    "        new_image2[:image2.shape[0], :image2.shape[1]] = image2\n",
    "        image2 = new_image2\n",
    "\n",
    "    offset = np.array(image1.shape)\n",
    "    if alignment == 'horizontal':\n",
    "        image = np.concatenate([image1, image2], axis=1)\n",
    "        offset[0] = 0\n",
    "    elif alignment == 'vertical':\n",
    "        image = np.concatenate([image1, image2], axis=0)\n",
    "        offset[1] = 0\n",
    "    else:\n",
    "        mesg = (\"plot_matches accepts either 'horizontal' or 'vertical' for \"\n",
    "                \"alignment, but '{}' was given. See \"\n",
    "                \"https://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.plot_matches \"  # noqa\n",
    "                \"for details.\").format(alignment)\n",
    "        raise ValueError(mesg)\n",
    "\n",
    "    ax.imshow(image, interpolation='nearest', cmap='gray')\n",
    "    ax.axis((0, image1.shape[1] + offset[1], image1.shape[0] + offset[0], 0))\n",
    "\n",
    "    for i in range(matches.shape[0]):\n",
    "        idx1 = matches[i, 0]\n",
    "        idx2 = matches[i, 1]\n",
    "\n",
    "        if matches_color is None:\n",
    "            color = np.random.rand(3)\n",
    "        else:\n",
    "            color = matches_color\n",
    "\n",
    "        ax.plot((keypoints1[idx1, 1], keypoints2[idx2, 1] + offset[1]),\n",
    "                (keypoints1[idx1, 0], keypoints2[idx2, 0] + offset[0]),\n",
    "                '-', color=color, linewidth=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## mutual matches ##\n",
    "def mutual_max(P) :\n",
    "    max0, max1 = P.max(1), P.max(0)\n",
    "    indices0, indices1 = max0.indices, max1.indices\n",
    "    mutual0 = torch.LongTensor(range(indices0.shape[0])) == indices1.gather(0, indices0)\n",
    "    mutual1 = torch.LongTensor(range(indices1.shape[0])) == indices0.gather(0, indices1)\n",
    "    zero = P.new_tensor(0)\n",
    "    mscores0 = torch.where(mutual0, max0.values.exp(), zero)\n",
    "    mscores1 = torch.where(mutual1, mscores0.gather(0, indices1), zero)\n",
    "\n",
    "    valid0 = mutual0 & (mscores0 > 0)\n",
    "    valid1 = mutual1 & valid0.gather(1, indices1)\n",
    "\n",
    "    indices0 = torch.where(valid0, indices0, indices0.new_tensor(-1))\n",
    "    indices1 = torch.where(valid1, indices1, indices1.new_tensor(-1))\n",
    "\n",
    "    # make norm\n",
    "    return indices0, indices1, mscores0, mscores1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_idx = 0\n",
    "p_idx = 0\n",
    "n_idx = 0\n",
    "\n",
    "qimage = [ cfg['qim_fname'](cfg, q_idx) ]\n",
    "qgnd_images = [ cfg['im_fname'](cfg,j) for j in [p_idx] ]\n",
    "qgndj_images = [ cfg['im_fname'](cfg,j) for j in [n_idx] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'j' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-0ffb66e1a8d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mscores_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscores_1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0munique_1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mselect_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqgnd_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mscores_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqgnd_scores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselect_2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mdesc_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqgnd_descriptors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mselect_2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'j' is not defined"
     ]
    }
   ],
   "source": [
    "q_descriptors, q_locations, q_scores = extract_interest_points_mac(net, qimage, 1024, transform)\n",
    "qgnd_descriptors, qgnd_locations, qgnd_scores = extract_interest_points_mac(net, qgnd_images, 1024, transform)\n",
    "qgndj_descriptors, qgndj_locations, qgndj_scores = extract_interest_points_mac(net, qgndj_images, 1024, transform)\n",
    "\n",
    "\n",
    "q_descriptors = q_descriptors.numpy()\n",
    "q_locations = q_locations.numpy()\n",
    "\n",
    "qgnd_descriptors = qgnd_descriptors.numpy()\n",
    "qgnd_locations = qgnd_locations.numpy()\n",
    "\n",
    "qgndj_descriptors = qgndj_descriptors.numpy()\n",
    "qgndj_locations = qgndj_locations.numpy()\n",
    "\n",
    "q_scores = q_scores.numpy()\n",
    "qgnd_scores = qgnd_scores.numpy()\n",
    "qgndj_scores = qgndj_scores.numpy()\n",
    "\n",
    "## OT\n",
    "\n",
    "select_1 = q_scores[0] > 25\n",
    "scores_1 = q_scores[0, select_1]\n",
    "desc_1 = q_descriptors[0, select_1, :]\n",
    "loc_1 = q_locations[0, select_1, :]\n",
    "loc_1, unique_1, _ = np.unique(loc_1, axis=0, return_index=True, return_inverse=True)\n",
    "desc_1 = desc_1[unique_1, :]\n",
    "scores_1 = scores_1[unique_1]\n",
    "\n",
    "select_2 = qgnd_scores[j] > 10\n",
    "scores_2 = qgnd_scores[j, select_2]\n",
    "desc_2 = qgnd_descriptors[j, select_2, :]\n",
    "loc_2 = qgnd_locations[j, select_2, :]\n",
    "loc_2, unique_2, _ = np.unique(loc_2, axis=0, return_index=True, return_inverse=True)\n",
    "desc_2 = desc_2[unique_2, :]\n",
    "scores_2 = scores_2[unique_2]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_a = desc_1.shape[0]\n",
    "N_b = desc_2.shape[0]\n",
    "\n",
    "a = np.ones(N_a)\n",
    "a = scores_1\n",
    "a_sum = np.sum(a)\n",
    "\n",
    "b = np.ones(N_b)\n",
    "b = scores_2\n",
    "b_sum = np.sum(b)\n",
    "\n",
    "a = np.concatenate((a, np.array([b_sum])))\n",
    "b = np.concatenate((b, np.array([a_sum])))\n",
    "\n",
    "a = a / np.sum(a)\n",
    "b = b / np.sum(b)\n",
    "\n",
    "# do we need + 1?\n",
    "\n",
    "P = ot.sinkhorn(a, b, M, 0.01, numItermax=2000)\n",
    "\n",
    "ind_0, ind_1, _, _ = mutual_max(P)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plot_matches(\n",
    "    ax,\n",
    "    img_1,\n",
    "    img_2,\n",
    "    loc_1,\n",
    "    loc_2,\n",
    "    max_match,\n",
    "    matches_color='b')\n",
    "ax.axis('off')\n",
    "ax.set_title('OT correspondences')\n",
    "plt.savefig('ot_' + str(loc_scale) + '_' + img_1_name + '_' + img_2_name + '_' + args.output_image, dpi=300)\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
